{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVNzKTFMT4FX",
        "outputId": "5c49a4d8-91cf-45d5-9b3a-45e9f596c6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print('Hello, World!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation\n",
        "Terminal command for installing pytorch for GPU."
      ],
      "metadata": {
        "id": "dKDI8r6eWck8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZtv2MdhVefn",
        "outputId": "b250cd9c-ed48-4ba5-fc5b-4111f27ab8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required packages."
      ],
      "metadata": {
        "id": "UttuGfdQkIIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-IFo246YVwU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if pytorch is utilizing the GPU."
      ],
      "metadata": {
        "id": "xmBOJCEGkMig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHyImHz6WDgv",
        "outputId": "569be808-241a-499e-b626-8d803fdb1371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.rand(m, n)` - Returns a $m \\times n$ tensor filled with random numbers from a uniform distribution on the interval $[0, 1)$."
      ],
      "metadata": {
        "id": "HP9Vv78rkfUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsMwHcB_WGd_",
        "outputId": "f6a9f9c1-0d9a-4b25-bd55-aa7e20be8740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8391, 0.8710, 0.4108],\n",
            "        [0.1367, 0.7966, 0.5889],\n",
            "        [0.1890, 0.5635, 0.9352],\n",
            "        [0.6978, 0.6866, 0.9521],\n",
            "        [0.0285, 0.7717, 0.3764]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tensor Basics"
      ],
      "metadata": {
        "id": "UFEjJl9kWufo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   `torch.empty(size)` - Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument `size`.\n",
        "*   `torch.ones(size)` - Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument `size`. \n",
        "*   `size (int...)` – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. \n",
        "*   `dtype (torch.dtype, optional)` – the desired data type of returned tensor. Default is `float32`.\n",
        "\n"
      ],
      "metadata": {
        "id": "CraXfUzmk6RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.empty(1)\n",
        "print(x1)\n",
        "\n",
        "x2 = torch.empty(2,2)\n",
        "print(x2)\n",
        "\n",
        "x3 = torch.ones(2,2)\n",
        "print(x3)\n",
        "\n",
        "x4 = torch.rand(2,2)\n",
        "print(x4)\n",
        "\n",
        "x5 = torch.ones(2,2, dtype=torch.int32)\n",
        "print(x5, x5.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB3S8hmZWasS",
        "outputId": "4e34580a-53e8-41e4-9c17-a5f28f57538e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7.4651e-35])\n",
            "tensor([[ 7.4667e-35,  0.0000e+00],\n",
            "        [-2.0000e+00,  1.7640e+00]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.5161, 0.3778],\n",
            "        [0.1474, 0.1048]])\n",
            "tensor([[1, 1],\n",
            "        [1, 1]], dtype=torch.int32) torch.int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic mathematical operations between tensors."
      ],
      "metadata": {
        "id": "OREuKpg_mB-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "print(x, y)\n",
        "\n",
        "z1 = x + y\n",
        "print(z1)\n",
        "\n",
        "z2 = torch.add(x, y) # same as z = x + y\n",
        "print(z2)\n",
        "\n",
        "x.add_(y) # x += y\n",
        "print(x)\n",
        "\n",
        "\n",
        "z3 = x * y\n",
        "print(z3)\n",
        "\n",
        "z4 = torch.mul(x, y) # same as z = x * y\n",
        "print(z4)\n",
        "\n",
        "z5 = x / y\n",
        "print(z5)\n",
        "\n",
        "z6 = torch.div(x, y) # same as z = x / y\n",
        "print(z6)\n",
        "\n",
        "z7 = x - y\n",
        "print(z7)\n",
        "\n",
        "z8 = torch.sub(x, y) # same as z = x - y\n",
        "print(z8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLgHpeMPYK0X",
        "outputId": "289000b5-a9a7-403d-a1a8-760897432cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4255, 0.6686],\n",
            "        [0.3715, 0.4163]]) tensor([[0.5311, 0.1669],\n",
            "        [0.2869, 0.2304]])\n",
            "tensor([[0.9566, 0.8355],\n",
            "        [0.6584, 0.6466]])\n",
            "tensor([[0.9566, 0.8355],\n",
            "        [0.6584, 0.6466]])\n",
            "tensor([[0.9566, 0.8355],\n",
            "        [0.6584, 0.6466]])\n",
            "tensor([[0.5081, 0.1394],\n",
            "        [0.1889, 0.1490]])\n",
            "tensor([[0.5081, 0.1394],\n",
            "        [0.1889, 0.1490]])\n",
            "tensor([[1.8011, 5.0057],\n",
            "        [2.2949, 2.8070]])\n",
            "tensor([[1.8011, 5.0057],\n",
            "        [2.2949, 2.8070]])\n",
            "tensor([[0.4255, 0.6686],\n",
            "        [0.3715, 0.4163]])\n",
            "tensor([[0.4255, 0.6686],\n",
            "        [0.3715, 0.4163]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing a matrix along rows and columns."
      ],
      "metadata": {
        "id": "DwrE3sLFqkOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "print(x[:, 0]) # x[:, i] -> all rows of ith column\n",
        "print(x[0, :]) # x[i, :] -> all columns of ith row\n",
        "print(x[1, 1]) # x[i, j] -> element at ith row and jth column\n",
        "print(x[1, 1].item()) # just get the value itself"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFA2nb0xZquq",
        "outputId": "b13b01cb-820c-4930-8ab2-f3131cf9fb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0785, 0.4065, 0.5383],\n",
            "        [0.1682, 0.6793, 0.0947],\n",
            "        [0.4998, 0.0489, 0.5228],\n",
            "        [0.8323, 0.8792, 0.6739],\n",
            "        [0.1683, 0.4357, 0.5308]])\n",
            "tensor([0.0785, 0.1682, 0.4998, 0.8323, 0.1683])\n",
            "tensor([0.0785, 0.4065, 0.5383])\n",
            "tensor(0.6793)\n",
            "0.6793379187583923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.randn(size)` - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n",
        "$Out_i \\sim \\mathcal{N}(0, 1)$\n",
        "\n",
        "* `Tensor.view(*shape)` - Returns a new tensor with the same data as the `self` tensor but of a different `shape`. The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride."
      ],
      "metadata": {
        "id": "jRK3v3d5szCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z1 = x.view(-1, 8) # the size -1 is inferred from other dimensions\n",
        "z2 = x.view(8, 2)\n",
        "print(x, y, z1, z2)\n",
        "print(x.size(), y.size(), z1.size(), z2.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17cNoXW7aaWr",
        "outputId": "bb98d68d-94e6-4d10-9fbf-3f09c15e33b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5247, -0.5801,  0.2874, -1.1761],\n",
            "        [-0.0789, -0.1503, -0.5431,  0.5938],\n",
            "        [ 0.0114, -0.6989,  0.2251,  0.7482],\n",
            "        [-2.1660, -0.2883, -0.8345, -0.7719]]) tensor([ 0.5247, -0.5801,  0.2874, -1.1761, -0.0789, -0.1503, -0.5431,  0.5938,\n",
            "         0.0114, -0.6989,  0.2251,  0.7482, -2.1660, -0.2883, -0.8345, -0.7719]) tensor([[ 0.5247, -0.5801,  0.2874, -1.1761, -0.0789, -0.1503, -0.5431,  0.5938],\n",
            "        [ 0.0114, -0.6989,  0.2251,  0.7482, -2.1660, -0.2883, -0.8345, -0.7719]]) tensor([[ 0.5247, -0.5801],\n",
            "        [ 0.2874, -1.1761],\n",
            "        [-0.0789, -0.1503],\n",
            "        [-0.5431,  0.5938],\n",
            "        [ 0.0114, -0.6989],\n",
            "        [ 0.2251,  0.7482],\n",
            "        [-2.1660, -0.2883],\n",
            "        [-0.8345, -0.7719]])\n",
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.from_numpy(ndarray) → Tensor` - Creates a `Tensor` from a `numpy.ndarray`. Both torch tensor version and numpy array version will share same memory location. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable.\n",
        "\n",
        "* `.numpy()` - Converts a `tensor` object into an `numpy.ndarray` object."
      ],
      "metadata": {
        "id": "naHMDjz6dReQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a =  torch.ones(5)\n",
        "print(a)\n",
        "\n",
        "a_np = a.numpy()\n",
        "print(a_np)\n",
        "\n",
        "a.add_(1) # a += 1\n",
        "print(a, a_np)\n",
        "\n",
        "b = np.ones(5)\n",
        "print(b)\n",
        "b_torch = torch.from_numpy(b)\n",
        "print(b_torch)\n",
        "\n",
        "b += 1\n",
        "print(b, b_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eJGSTmycuhr",
        "outputId": "f059a00e-2e46-4804-aa3f-da4dd62ab468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Gradient Calculation With Autograd"
      ],
      "metadata": {
        "id": "iB6vtvY0eq2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKWQtqoOdOxA",
        "outputId": "b26d648b-64f7-47df-b6c8-bf2a03ee6515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\n",
        "\n",
        "`requires_grad = True` - Tracks all operations on the tensor.\n",
        "\n",
        "`backward()` - Computes the sum of gradients of given tensors with respect to graph leaves. If a Tensor is non-scalar (more than 1 element), we need to specify arguments for backward() that is a tensor of matching shape.\n",
        "\n",
        "`grad` - Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
        "\n",
        "Uses vector Jacobian product, a.k.a. computing partial derivatives while applying the chain rule."
      ],
      "metadata": {
        "id": "BnGJxWl4f5z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = 2 * y**2\n",
        "print(z)\n",
        "\n",
        "# z = z.mean()\n",
        "# print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 1.0, 0.001, 1.1], dtype=torch.float32)\n",
        "# z.backward() # dz/dx\n",
        "z.backward(v)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk4IrD42el56",
        "outputId": "b798c903-7617-47fe-e5fa-31c0432ae553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3., 3.], grad_fn=<AddBackward0>)\n",
            "tensor([18., 18., 18., 18., 18.], grad_fn=<MulBackward0>)\n",
            "tensor([ 6.0000, 16.8000, 16.8000,  4.8120, 18.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.no_grad()` - Context-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when we are sure that we will not call `Tensor.backward()`. It will reduce memory consumption for computations that would otherwise have `requires_grad=True`. In this mode, the result of every computation will have `requires_grad=False`, even when the inputs have `requires_grad=True`. This context manager is thread local; it will not affect computation in other threads."
      ],
      "metadata": {
        "id": "863-Sf-3xOkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(3, requires_grad=True)\n",
        "print(x)\n",
        "# x.requires_grad_(False)\n",
        "# x_nograd = x.detach()\n",
        "# print(x_nograd)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = x + 2\n",
        "    print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc8LuxLcfRi7",
        "outputId": "91652eff-9fa8-48b8-b040-b7de018135df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.], requires_grad=True)\n",
            "tensor([3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are cases where it may be necessary to zero-out the gradients of a tensor, *e.g.* when we start our training loop."
      ],
      "metadata": {
        "id": "oxXrCxYpzvK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.ones(4, requires_grad=True)\n",
        "for epoch in range(3):\n",
        "    y = (w * 3).sum()\n",
        "    y.backward()\n",
        "    print(w.grad)\n",
        "    w.grad.zero_() # empty gradients, otherwise the values will accumulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb3TUT4n8oaK",
        "outputId": "4681f596-a320-4454-8c6d-2968cc896b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Backpropagation"
      ],
      "metadata": {
        "id": "5CC90egE-6ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.Tensor.backward()` - Computes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires `gradient`, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. `self`."
      ],
      "metadata": {
        "id": "LVyVhJH3KB9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass to compute loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "print(loss)\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# update weights, this operation should not be part of the computational graph\n",
        "with torch.no_grad():\n",
        "    w -= 0.01 * w.grad\n",
        "w.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS_-EzNF9jN6",
        "outputId": "8fdae880-1f2e-4e28-a088-ea10353915e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Gradient Descent with Autograd and Backpropagation"
      ],
      "metadata": {
        "id": "fGe_dVuWYvEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A manual implementation of Gradient Descent.\n",
        "The function that we are trying to determine is a Linear function,\n",
        "$$f(w, X) = w \\cdot X$$\n",
        "where, the value of $w$ should be 2.\n",
        "The metric for loss is the Mean Squared Error (MSE) loss.\n",
        "$$J = \\frac{1}{N}\\sum_i(w \\cdot x_i - y_i)^2$$\n",
        "\n",
        "The gradient w.r.t the loss is,\n",
        "$$\\frac{\\partial J}{\\partial w} = \\frac{2}{N} \\sum_i x_i (w \\cdot x_i - y_i)$$"
      ],
      "metadata": {
        "id": "v_EbisFgLPWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20], dtype=np.float32)\n",
        "\n",
        "w = random.uniform(0, 10) # random initialization\n",
        "# print(w)\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "def loss(y, y_hat):\n",
        "    return ((y_hat - y)**2).mean()\n",
        "\n",
        "def gradient(x, y, y_hat):\n",
        "    return ((2 * x) * (y_hat - y)).mean()\n",
        "\n",
        "print(f'Prediction pre-training: f(5) = {forward(5)}')\n",
        "\n",
        "lr = 0.01\n",
        "i = 10\n",
        "\n",
        "for epoch in range(i):\n",
        "    Y_hat = forward(X)\n",
        "    J = loss(Y, Y_hat)\n",
        "    dw = gradient(X, Y, Y_hat)\n",
        "    w -= lr * dw\n",
        "    print(f'Epoch {epoch}: w = {w}, loss = {J}')\n",
        "\n",
        "print(f'Prediction post-training: f(5) = {forward(5)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBNQduN796Gg",
        "outputId": "981fe6ae-ce14-4511-bb53-87eaadb0669e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction pre-training: f(5) = 9.4216768617723\n",
            "Epoch 0: w = 1.9733971380252362, loss = 0.5150647759437561\n",
            "Epoch 1: w = 1.9938813637752058, loss = 0.027246981859207153\n",
            "Epoch 2: w = 1.9985926904339315, loss = 0.001441337401047349\n",
            "Epoch 3: w = 1.9996762668896437, loss = 7.624272257089615e-05\n",
            "Epoch 4: w = 1.999925597402823, loss = 4.036734480905579e-06\n",
            "Epoch 5: w = 1.9999828750822664, loss = 2.1303458197508007e-07\n",
            "Epoch 6: w = 1.9999960930082918, loss = 1.1345036909915507e-08\n",
            "Epoch 7: w = 1.999999160263312, loss = 6.109601713433221e-10\n",
            "Epoch 8: w = 1.9999997818205768, loss = 2.5131896563834744e-11\n",
            "Epoch 9: w = 1.999999959680833, loss = 2.120259480140052e-12\n",
            "Prediction post-training: f(5) = 9.999999798404165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The manually implemented gradient computation is replaced with `autograd`'s backward pass `backward`."
      ],
      "metadata": {
        "id": "GDrJ-UXKOPZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16, 18, 20], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(random.uniform(0, 10), dtype=torch.float32, requires_grad=True) # random initialization\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "def loss(y, y_hat):\n",
        "    return ((y_hat - y)**2).mean()\n",
        "\n",
        "print(f'Prediction pre-training: f(5) = {forward(5)}')\n",
        "\n",
        "lr = 0.01\n",
        "i = 100\n",
        "\n",
        "for epoch in range(i):\n",
        "    Y_hat = forward(X)\n",
        "    J = loss(Y, Y_hat)\n",
        "    J.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= lr * w.grad\n",
        "    w.grad.zero_()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item()}, loss = {J.item()}')\n",
        "\n",
        "print(f'Prediction post-training: f(5) = {forward(5).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGlBnRJ-bYbq",
        "outputId": "5e3aed72-7bba-4459-8021-66e5c7aacd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction pre-training: f(5) = 5.2617716789245605\n",
            "Epoch 0: w = 1.7820414304733276, loss = 34.574241638183594\n",
            "Epoch 10: w = 1.9999998807907104, loss = 7.685230056508896e-12\n",
            "Epoch 20: w = 2.0, loss = 0.0\n",
            "Epoch 30: w = 2.0, loss = 0.0\n",
            "Epoch 40: w = 2.0, loss = 0.0\n",
            "Epoch 50: w = 2.0, loss = 0.0\n",
            "Epoch 60: w = 2.0, loss = 0.0\n",
            "Epoch 70: w = 2.0, loss = 0.0\n",
            "Epoch 80: w = 2.0, loss = 0.0\n",
            "Epoch 90: w = 2.0, loss = 0.0\n",
            "Prediction post-training: f(5) = 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Pipeline: Model, Loss, and Optimizer"
      ],
      "metadata": {
        "id": "PUxhW8zGhf4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)` - Applies a linear transformation to the incoming data: $$y = x W^T + b$$\n",
        "* `in_features (int)` – size of each input sample\n",
        "* `out_features (int)` – size of each output sample\n",
        "* `bias (bool)` – If set to `False`, the layer will not learn an additive bias. Default: `True`\n",
        "\n",
        "`weight (torch.Tensor)` – the learnable weights of the module of shape `(out_features,in_features)`. The values are initialized from $w \\sim \\mathcal{U}(-\\sqrt k, \\sqrt k)$, where, $k = \\frac{1}{in\\_features}$. The range is same for `bias`."
      ],
      "metadata": {
        "id": "PbW68g09QKFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16], [18], [20]], dtype=torch.float32)\n",
        "\n",
        "# w = torch.tensor([random.uniform(0, 10)], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "X_test = torch.tensor([69], dtype=torch.float32)\n",
        "\n",
        "model = nn.Linear(X.shape[1], X.shape[1])\n",
        "\n",
        "print(f'Prediction pre-training: f({X_test.item()}) = {model(X_test).item()}')\n",
        "\n",
        "lr = 0.01\n",
        "i = 500\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
        "for epoch in range(i):\n",
        "    Y_hat = model(X)\n",
        "    J = loss(Y, Y_hat)\n",
        "    J.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if epoch % 10 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'Epoch {epoch}: w = {w[0][0].item()}, b = {b.item()}, loss = {J.item()}')\n",
        "\n",
        "print(f'Prediction post-training: f({X_test.item()}) = {model(X_test).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3u2dzhKdf-d",
        "outputId": "8a5e1451-3985-491f-bebb-f081d963e619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction pre-training: f(69.0) = 55.469512939453125\n",
            "Epoch 0: w = 1.6658542156219482, b = 0.6429587006568909, loss = 49.150882720947266\n",
            "Epoch 10: w = 1.9067645072937012, b = 0.6490885615348816, loss = 0.09105506539344788\n",
            "Epoch 20: w = 1.910606861114502, b = 0.6223392486572266, loss = 0.08370493352413177\n",
            "Epoch 30: w = 1.9142907857894897, b = 0.5966922640800476, loss = 0.07694792747497559\n",
            "Epoch 40: w = 1.9178229570388794, b = 0.5721022486686707, loss = 0.07073663920164108\n",
            "Epoch 50: w = 1.9212095737457275, b = 0.5485256314277649, loss = 0.06502646207809448\n",
            "Epoch 60: w = 1.9244564771652222, b = 0.5259206295013428, loss = 0.05977732688188553\n",
            "Epoch 70: w = 1.9275696277618408, b = 0.5042470693588257, loss = 0.05495183914899826\n",
            "Epoch 80: w = 1.9305545091629028, b = 0.483466774225235, loss = 0.0505160391330719\n",
            "Epoch 90: w = 1.933416485786438, b = 0.4635428488254547, loss = 0.04643832892179489\n",
            "Epoch 100: w = 1.9361604452133179, b = 0.44443994760513306, loss = 0.042689695954322815\n",
            "Epoch 110: w = 1.938791275024414, b = 0.42612430453300476, loss = 0.03924364224076271\n",
            "Epoch 120: w = 1.9413137435913086, b = 0.4085635244846344, loss = 0.03607577458024025\n",
            "Epoch 130: w = 1.9437321424484253, b = 0.39172640442848206, loss = 0.033163562417030334\n",
            "Epoch 140: w = 1.9460511207580566, b = 0.37558311223983765, loss = 0.03048662282526493\n",
            "Epoch 150: w = 1.9482743740081787, b = 0.36010509729385376, loss = 0.02802557870745659\n",
            "Epoch 160: w = 1.9504059553146362, b = 0.3452649414539337, loss = 0.025763291865587234\n",
            "Epoch 170: w = 1.9524497985839844, b = 0.33103641867637634, loss = 0.023683618754148483\n",
            "Epoch 180: w = 1.9544092416763306, b = 0.3173941969871521, loss = 0.021771792322397232\n",
            "Epoch 190: w = 1.9562880992889404, b = 0.3043142259120941, loss = 0.020014313980937004\n",
            "Epoch 200: w = 1.9580894708633423, b = 0.29177325963974, loss = 0.018398713320493698\n",
            "Epoch 210: w = 1.9598166942596436, b = 0.2797491252422333, loss = 0.01691347546875477\n",
            "Epoch 220: w = 1.9614726305007935, b = 0.2682204842567444, loss = 0.015548193827271461\n",
            "Epoch 230: w = 1.9630604982376099, b = 0.25716695189476013, loss = 0.014293111860752106\n",
            "Epoch 240: w = 1.9645826816558838, b = 0.2465689331293106, loss = 0.013139372691512108\n",
            "Epoch 250: w = 1.9660422801971436, b = 0.23640765249729156, loss = 0.01207871176302433\n",
            "Epoch 260: w = 1.9674416780471802, b = 0.22666513919830322, loss = 0.011103619821369648\n",
            "Epoch 270: w = 1.9687834978103638, b = 0.2173241525888443, loss = 0.010207355953752995\n",
            "Epoch 280: w = 1.9700698852539062, b = 0.20836806297302246, loss = 0.009383315220475197\n",
            "Epoch 290: w = 1.9713033437728882, b = 0.1997811198234558, loss = 0.008625904098153114\n",
            "Epoch 300: w = 1.972485899925232, b = 0.19154801964759827, loss = 0.007929584011435509\n",
            "Epoch 310: w = 1.973619818687439, b = 0.18365418910980225, loss = 0.007289488799870014\n",
            "Epoch 320: w = 1.9747068881988525, b = 0.1760856807231903, loss = 0.0067010559141635895\n",
            "Epoch 330: w = 1.9757492542266846, b = 0.16882909834384918, loss = 0.006160144228488207\n",
            "Epoch 340: w = 1.9767487049102783, b = 0.16187156736850739, loss = 0.005662873387336731\n",
            "Epoch 350: w = 1.9777069091796875, b = 0.1552007496356964, loss = 0.005205766297876835\n",
            "Epoch 360: w = 1.9786256551742554, b = 0.14880485832691193, loss = 0.004785518161952496\n",
            "Epoch 370: w = 1.9795063734054565, b = 0.14267249405384064, loss = 0.0043992167338728905\n",
            "Epoch 380: w = 1.9803509712219238, b = 0.1367928683757782, loss = 0.004044136963784695\n",
            "Epoch 390: w = 1.9811607599258423, b = 0.13115553557872772, loss = 0.003717669053003192\n",
            "Epoch 400: w = 1.9819371700286865, b = 0.12575049698352814, loss = 0.0034175596665591\n",
            "Epoch 410: w = 1.9826815128326416, b = 0.12056826055049896, loss = 0.0031416963320225477\n",
            "Epoch 420: w = 1.9833952188491821, b = 0.11559958755970001, loss = 0.0028880818281322718\n",
            "Epoch 430: w = 1.9840794801712036, b = 0.11083567142486572, loss = 0.002654942451044917\n",
            "Epoch 440: w = 1.9847356081008911, b = 0.10626807063817978, loss = 0.0024406390730291605\n",
            "Epoch 450: w = 1.9853646755218506, b = 0.10188870131969452, loss = 0.0022436310537159443\n",
            "Epoch 460: w = 1.985967755317688, b = 0.09768979996442795, loss = 0.00206248601898551\n",
            "Epoch 470: w = 1.9865460395812988, b = 0.09366395324468613, loss = 0.001896020257845521\n",
            "Epoch 480: w = 1.9871004819869995, b = 0.0898040160536766, loss = 0.0017429515719413757\n",
            "Epoch 490: w = 1.987632155418396, b = 0.0861031636595726, loss = 0.0016022674972191453\n",
            "Prediction post-training: f(69.0) = 137.26123046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Linear Regression\n",
        "`sklearn.datasets.make_regression` - Generate a random regression problem. The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. `noise` is the standard deviation of the Gaussian noise applied to the output.\n",
        "\n",
        "`torch.optim.SGD` - Implements stochastic gradient descent (optionally with momentum)."
      ],
      "metadata": {
        "id": "7t_Rev1bmrzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "Y = Y.view(Y.shape[0], 1) # reshape row vector to column vector\n",
        "\n",
        "model = nn.Linear(X.shape[1], X.shape[0])\n",
        "\n",
        "lr = 0.01\n",
        "i = 27000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(i):\n",
        "    Y_hat = model(X)\n",
        "    J = loss(Y, Y_hat)\n",
        "    J.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if epoch % 1000 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'Epoch {epoch}: loss = {J.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bRfnK_skdIs",
        "outputId": "0d30bde3-f05a-4097-cc5f-a7de66ab89cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([100, 100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 2137.387451171875\n",
            "Epoch 1000: loss = 1623.661865234375\n",
            "Epoch 2000: loss = 1256.07861328125\n",
            "Epoch 3000: loss = 992.5604248046875\n",
            "Epoch 4000: loss = 803.3068237304688\n",
            "Epoch 5000: loss = 667.161376953125\n",
            "Epoch 6000: loss = 569.0682983398438\n",
            "Epoch 7000: loss = 498.28985595703125\n",
            "Epoch 8000: loss = 447.1520080566406\n",
            "Epoch 9000: loss = 410.1590881347656\n",
            "Epoch 10000: loss = 383.36822509765625\n",
            "Epoch 11000: loss = 363.94573974609375\n",
            "Epoch 12000: loss = 349.8515930175781\n",
            "Epoch 13000: loss = 339.6151428222656\n",
            "Epoch 14000: loss = 332.1745910644531\n",
            "Epoch 15000: loss = 326.7623596191406\n",
            "Epoch 16000: loss = 322.82293701171875\n",
            "Epoch 17000: loss = 319.9538269042969\n",
            "Epoch 18000: loss = 317.863037109375\n",
            "Epoch 19000: loss = 316.3387451171875\n",
            "Epoch 20000: loss = 315.2269592285156\n",
            "Epoch 21000: loss = 314.41571044921875\n",
            "Epoch 22000: loss = 313.8235168457031\n",
            "Epoch 23000: loss = 313.39111328125\n",
            "Epoch 24000: loss = 313.07525634765625\n",
            "Epoch 25000: loss = 312.8445129394531\n",
            "Epoch 26000: loss = 312.67584228515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Tensor.detach()` - Returns a new Tensor, detached from the current computational graph. The result will never require gradient."
      ],
      "metadata": {
        "id": "qnTN9NU8sZ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, Y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "VhU3Ltyaorl9",
        "outputId": "e12d3adb-5972-4cfd-92a3-457b14e56cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Logistic Regression\n",
        "`sklearn.datasets.load_breast_cancer` - Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset (classification).\n",
        "`sklearn.preprocessing.StandardScaler` - Standardize features by removing the mean and scaling to unit variance.\n",
        "The standard score of a sample x is calculated as:\n",
        "$$z = \\frac{x-\\mu}{\\sigma}$$ where, where $\\mu$ is the mean and $\\sigma$ is the variance of the training samples. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with $\\mu = 0$ and $\\sigma = 1$).\n",
        "\n",
        "* `fit_transform` - Fit to data, then transform it.\n",
        "* `transform` - Perform standardization by centering and scaling.\n",
        "\n",
        "`super(class_name, self)` - function which allows us to access temporary object of the super class.\n",
        "\n",
        "`torch.sigmoid` - Computes the expit (also known as the logistic sigmoid function) of the elements of input.\n",
        "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
        "\n",
        "`torch.round(input)` - Rounds elements of `input` to the nearest integer. This function implements the “round half to even” to break ties when a number is equidistant from two integers."
      ],
      "metadata": {
        "id": "Z9DQVIOMp_xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "X, Y = data.data, data.target\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale so that mean = 0 and variance = 1\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
        "Y_train_tensor = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Y_test_tensor = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "Y_train_tensor = Y_train_tensor.view(Y_train_tensor.shape[0], 1)\n",
        "Y_test_tensor = Y_test_tensor.view(Y_test_tensor.shape[0], 1)\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super(LogisticRegression, self).__init__() # super constructor\n",
        "        self.linear = nn.Linear(n_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_hat = torch.sigmoid(self.linear(x))\n",
        "        return y_hat\n",
        "\n",
        "model = LogisticRegression(X.shape[1])\n",
        "\n",
        "lr = 0.01\n",
        "i = 31000\n",
        "\n",
        "loss = nn.BCELoss() # shows nan with Binary Cross Entropy loss (nn.BCELoss()), if (Y_hat, Y_train_tensor) serial not maintained\n",
        "# loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(i):\n",
        "    Y_hat = model(X_train_tensor)\n",
        "    J = loss(Y_hat, Y_train_tensor)\n",
        "    J.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if epoch % 1000 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'Epoch {epoch}: loss = {J.item()}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    Y_hat = model(X_test_tensor)\n",
        "    print(f'Predicted classes: {Y_hat.round().view(1,-1)}')\n",
        "    print(f'Actual classes: {Y_test_tensor.view(1,-1)}')\n",
        "    accuracy = Y_hat.round().eq(Y_test_tensor).sum() / float(Y_test_tensor.shape[0])\n",
        "    print(f'Accuracy: {accuracy.item() * 100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iJxjdOlpKWX",
        "outputId": "a3659644-376d-4996-8c15-e91709c3b9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 0.8484773635864258\n",
            "Epoch 1000: loss = 0.10528001934289932\n",
            "Epoch 2000: loss = 0.08590676635503769\n",
            "Epoch 3000: loss = 0.07753192633390427\n",
            "Epoch 4000: loss = 0.07255382835865021\n",
            "Epoch 5000: loss = 0.06913077086210251\n",
            "Epoch 6000: loss = 0.06657207012176514\n",
            "Epoch 7000: loss = 0.06455431878566742\n",
            "Epoch 8000: loss = 0.06290357559919357\n",
            "Epoch 9000: loss = 0.06151673570275307\n",
            "Epoch 10000: loss = 0.060328226536512375\n",
            "Epoch 11000: loss = 0.05929381027817726\n",
            "Epoch 12000: loss = 0.05838232859969139\n",
            "Epoch 13000: loss = 0.057571135461330414\n",
            "Epoch 14000: loss = 0.05684318020939827\n",
            "Epoch 15000: loss = 0.05618511512875557\n",
            "Epoch 16000: loss = 0.055586591362953186\n",
            "Epoch 17000: loss = 0.05503927543759346\n",
            "Epoch 18000: loss = 0.05453627556562424\n",
            "Epoch 19000: loss = 0.05407189205288887\n",
            "Epoch 20000: loss = 0.05364137142896652\n",
            "Epoch 21000: loss = 0.05324067920446396\n",
            "Epoch 22000: loss = 0.052866559475660324\n",
            "Epoch 23000: loss = 0.05251595377922058\n",
            "Epoch 24000: loss = 0.05218644067645073\n",
            "Epoch 25000: loss = 0.051875658333301544\n",
            "Epoch 26000: loss = 0.05158192664384842\n",
            "Epoch 27000: loss = 0.05130348354578018\n",
            "Epoch 28000: loss = 0.051038939505815506\n",
            "Epoch 29000: loss = 0.0507870577275753\n",
            "Epoch 30000: loss = 0.05054660141468048\n",
            "Predicted classes: tensor([[1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "         1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "         1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "         1., 1., 0., 1., 0., 0.]])\n",
            "Actual classes: tensor([[1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "         1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "         1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "         1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "         1., 1., 0., 1., 1., 0.]])\n",
            "Accuracy: 98.24561476707458%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v0Lca4IluZc",
        "outputId": "d81adcf3-1f25-47e4-cb1c-ebf1d222b4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30) (569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CM = confusion_matrix(Y_test_tensor.round(), Y_hat.round(),labels=[0.,1.])\n",
        "print(CM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GsOsq7cwOjV",
        "outputId": "c4d352c3-cfa6-4b0d-c3ce-d3d449842b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[42  1]\n",
            " [ 1 70]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 0 -> 'malignant'\n",
        "* 1 -> 'benign'"
      ],
      "metadata": {
        "id": "GTFMpPXPx5a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(CM,annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "T0uwbG80wn5I",
        "outputId": "042e50ef-1f2f-48ce-b605-c2ed4662a62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0b797190d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAFpCAYAAAA7lisxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATi0lEQVR4nO3df7DcVXnH8feTQBCoEAJpjMQWqojFdoAORRgUEdDywxqmtRGrMTCh15mKI8VOiVXbsdURZ0RrO9bxImJEBVPUSUarbYxQYAYiUSKKsYIpGRPyg5BEfhSbH/v0j2zxNuTuTbK5e3IO7xfznbv73d2z5w/mk2eec77fjcxEkjRYE0pPQJKeiwxfSSrA8JWkAgxfSSrA8JWkAgxfSSrA8JWkvRARJ0bE8hHH4xFxVURMiYjFEfFg9+9RPcdxn68k7ZuImAisAV4BvAPYlJnXRsQ84KjMvGa0z1r5StK+Ow/4WWauAmYC87vn5wOX9Pqg4StJ++5S4Obu42mZubb7eB0wrdcHx73tsO7sc+xr6Flm3PNg6SnoALR965rod4xtG1f2lTmTpr747cDQiFPDmTm86/siYhLwCPDyzFwfEVsyc/KI1zdn5qh934P6maQkHXA6O/r6eDdonxW2u3Eh8P3MXN99vj4ipmfm2oiYDmzo9WHbDpK0b97Mr1oOAIuAOd3Hc4CFvT5s5SupLdkZ96+IiMOB1wJvH3H6WmBBRMwFVgGzeo1h+EpqS2f8wzcznwKO3uXcY+zc/bBHDF9JTckBVL77gz1fSSrAyldSWwbQdtgfDF9Jbamk7WD4SmpLn/t8B8XwldSWSipfF9wkqQArX0ltccFNkgavln2+hq+ktlj5SlIBlVS+LrhJUgFWvpLa4j5fSSqgkraD4SupLZUsuNnzlaQCrHwltcW2gyQVUEnbwfCV1JRMdztI0uBV0nZwwU2SCrDyldQWe76SVEAlbQfDV1JbvLxYkgqopPJ1wU2SCrDyldQWF9wkqYBK2g6Gr6S2VFL52vOVpAKsfCW1pZLK1/CV1BRvrCNJJVj5SlIBlex2cMFNkgqw8pXUFtsOklRAJW0Hw1dSWyqpfO35SmpLdvo79kBETI6IWyPiJxGxIiLOjIgpEbE4Ih7s/j2q1xiGryTtvU8A38rMlwEnAyuAecCSzDwBWNJ9PirbDpLaMs5th4g4EjgbuAwgM7cCWyNiJnBO923zgduBa0Ybx/CV1Jbx7/keDzwK3BgRJwPfA94FTMvMtd33rAOm9RrEtoOktvTZ842IoYhYNuIY2uUbDgJ+D/hUZp4KPMUuLYbMTCB7TdPKV5JGyMxhYLjHW1YDqzNzaff5rewM3/URMT0z10bEdGBDr++x8pXUlk6nv2MMmbkO+HlEnNg9dR7wY2ARMKd7bg6wsNc4Vr6S2jKYiyzeCXwxIiYBK4HL2VnMLoiIucAqYFavAQxfSW0ZwEUWmbkcOG03L523p2MYvpLaUsnlxfZ8JakAK19Jbank3g6Gr6S2GL6SVED2vLbhgGH4SmpLJZWvC26SVICVr6S2VFL5Gr6S2lLJPl/DV1JbKql87flKUgFWvpLa4lYzSSqgkraD4SupLYavJBVQyW4HF9wkqQArX0lNyY4LbpI0ePZ8JamASnq+hq+ktlTSdnDBTZIKsPKV1BZ7vpJUgOErSQVUcm8He76SVICV73iaMIGjhz/Njo0b2TLvPRz5/vdy8Iknktt3sG3FCh7/6HWwY0fpWaqQ64ev4+KLzmfDoxs55dTzSk+nHZW0Hax8x9Fhb/xjtq9a9czzpxd/m41vfRuPXXY5ccghHPr6iwvOTqV9/vMLuPj1byk9jfZ0sr9jQAzfcTJh6lQOOfMMnv7GN545t/Wepc883rZiBROnTi0xNR0g7rxrKZs2byk9jfZkp79jQMZsO0TEy4CZwLHdU2uARZm5YjwnVrsj3nklT3zq00w47LBnvzhxIof+wet4/B//afATk1rXwkUWEXENcAsQwHe7RwA3R8S8Hp8biohlEbHsprWP7M/5VuGQM8+ks3kz23/6092+fsTVf8HWH9zPtvt/OOCZSTpQjFX5zgVenpnbRp6MiI8BDwDX7u5DmTkMDAOsO/ucOv4Z2o8O/t3f4ZCzzmLqGWfApElMOPwwjnzfe/nFBz/E4ZfNYcLkyWx53/tLT1NqUlay4DZW+HaAFwKrdjk/vfuaduPJ4et5cvh6ACadcgqHXfomfvHBD3HoxRdzyOm/z6arrq5mL6JUnUraDmOF71XAkoh4EPh599xvAC8BrhzPibXoiHdfzY716zj6U/8MwC/vuIOn5n++8KxUyhdu+iSvPvtMjjlmCg+vXMYH/u6j3Pi5W0pPq34t3NUsM78VES8FTuf/L7jdm5luUN0DW5cvZ+vy5QCsP9e9nPqVt85+R+kpqKAxdztkZge4ZwBzkaT+NdJ2kKS6NLLgJkl1sfKVpAIqWXDz8mJJKsDKV1JbBtB2iIiHgSeAHcD2zDwtIqYAXwaOAx4GZmXm5tHGsPKV1JTsdPo69sJrMvOUzDyt+3wesCQzTwCWdJ+PyvCV1JZyt5ScCczvPp4PXNLrzYavpLb0Gb4jbwzWPYZ28y0J/HtEfG/E69Myc2338TpgWq9p2vOVpBFG3hish1dm5pqI+HVgcUT8ZJcxMiJ6ltGGr6S2DGCrWWau6f7dEBFfY+ctGNZHxPTMXBsR04ENvcaw7SCpLePc842IwyPi+f/3GHgd8CNgETCn+7Y5wMJe41j5SmpKjv9Ws2nA1yICdmbol7o3IbsXWBARc9l5G95ZvQYxfCVpL2TmSuDk3Zx/DNjjWxcavpLa4r0dJKkA72omSQVY+UpSAZWEr1vNJKkAK19JTclKfhnc8JXUlkraDoavpLYYvpI0eAO4wm2/cMFNkgqw8pXUlkoqX8NXUlvquMDN8JXUFnu+kqRRWflKakslla/hK6kt9nwlafBq6fkavpLaUknl64KbJBVg5SupKbYdJKmEStoOhq+kpqThK0kFVBK+LrhJUgFWvpKaYttBkkowfCVp8GqpfO35SlIBVr6SmlJL5Wv4SmqK4StJJWSUnsEeMXwlNaWWytcFN0kqwMpXUlOyY9tBkgaulraD4SupKemCmyQNXi2VrwtuklSA4SupKdmJvo49FRETI+K+iPh69/nxEbE0Ih6KiC9HxKRenzd8JTUls79jL7wLWDHi+UeAj2fmS4DNwNxeHzZ8JTVlEJVvRMwALgY+030ewLnArd23zAcu6TWG4StJI0TEUEQsG3EM7eZt/wD8Fb+6e/DRwJbM3N59vho4ttf3uNtBUlP6vcgiM4eB4dFej4jXAxsy83sRcc6+fo/hK6kpe9m33RdnAW+IiIuA5wFHAJ8AJkfEQd3qdwawptcgth0kNWW8e76Z+Z7MnJGZxwGXAt/JzLcAtwFv7L5tDrCw1ziGr6SmZEZfRx+uAa6OiIfY2QO+odebbTtI0j7KzNuB27uPVwKn7+lnDV9JTanl8mLDV1JTOt5YR5IGz7uaSVIBtdxM3d0OklSAla+kpgzgIov9wvCV1JRa2g6Gr6Sm1LLbwZ6vJBVg5SupKW41k6QCXHCTpAJq6fkavpKaUkvbwQU3SSrAyldSU+z5ds2458Hx/gpV6OlH7iw9BTXKnq8kFVBLz9fwldSUWipfF9wkqQArX0lNqWS9zfCV1JZa2g6Gr6Sm1LLgZs9Xkgqw8pXUlEp+Od7wldSWpI62g+ErqSmdSrY7GL6SmtKppPJ1wU2SCrDyldQUe76SVIC7HSSpgFoqX3u+klSAla+kpth2kKQCDF9JKqCWnq/hK6kpnTqy1wU3SSrByldSU7y8WJIKyD6PsUTE8yLiuxHxg4h4ICI+0D1/fEQsjYiHIuLLETGp1ziGr6SmdPo89sD/AOdm5snAKcAFEXEG8BHg45n5EmAzMLfXIIavpKZ0Ivo6xpI7Pdl9enD3SOBc4Nbu+fnAJb3GMXwlaS9FxMSIWA5sABYDPwO2ZOb27ltWA8f2GsPwldSUfnu+ETEUEctGHEPP+o7MHZl5CjADOB142d7O090OkprS7xVumTkMDO/he7dExG3AmcDkiDioW/3OANb0+qyVr6SmdKK/YywRMTUiJncfHwq8FlgB3Aa8sfu2OcDCXuNY+UrS3pkOzI+IiewsYBdk5tcj4sfALRHxQeA+4IZegxi+kpoy3hdZZOb9wKm7Ob+Snf3fPWL4SmpKJT9ebPhKakstN9YxfCU1pZb7+brbQZIKsPKV1BR7vpJUgD1fSSqglp6v4SupKbWErwtuklSAla+kpqQ9X0kavFraDoavpKbUEr72fCWpACtfSU3xIgtJKsCLLCSpgFp6voavpKbUEr4uuElSAVa+kprigpskFeCCmyQVUEvP1/CV1JRa2g4uuElSAVa+kprSqaT2NXwlNcWeryQVUEfda89Xkoqw8pXUFNsOklSAF1lIUgHudpCkAuqIXhfcJKkIK19JTXHBTZIKsOcrSQXUEb2Gr6TG1NJ2cMFNkgqw8pXUFHu+klRAHdFr20FSYzp9HmOJiBdFxG0R8eOIeCAi3tU9PyUiFkfEg92/R/Uax/CVpL2zHXh3Zp4EnAG8IyJOAuYBSzLzBGBJ9/moDF9JTck+/xtz/My1mfn97uMngBXAscBMYH73bfOBS3qNY/hKakq/bYeIGIqIZSOOodG+KyKOA04FlgLTMnNt96V1wLRe83TBTVJT+t3tkJnDwPBY74uIXwO+AlyVmY9H/OpelpmZEdFzIla+kpqSfR57IiIOZmfwfjEzv9o9vT4ipndfnw5s6DWG4StJeyF2lrg3ACsy82MjXloEzOk+ngMs7DWObYcBuH74Oi6+6Hw2PLqRU049r/R0VNB/rVrNX/7Nh595vvqRtVx5xWzecOH5vPv9H+aRdet54Qumcd3fv4cjj3h+wZnWawAXWZwFzAZ+GBHLu+f+GrgWWBARc4FVwKxeg0Tm+E70oEnH1rLnedy86pWv4Mknn+LGGz9h+HY9/cidpadQ3I4dOzj3ktncfP3HufkrX+fII57PFbNn8ZmbFvD4E09w9Z/PLT3FgTv4mN/q+0eA/uy4P+krc65/+F8G8kNEth0G4M67lrJp85bS09AB5p5ly3nRsdN54QumcduddzPzwvMBmHnh+XznjrsLz65e473VbH+x7SAV8s0l/8FF578agMc2b2HqMVMAOOboo3jMf6z3WfN3NYuIy3u89sw+uU7nqX39CqlZ27Zt4/a7lvK6c1/1rNcigpHbltSmftoOHxjthcwczszTMvO0CRMO7+MrpDbdec8yfvulL+aYKTsv/z/6qMk8unETAI9u3MSUyUeWnF7Vmmg7RMT9o73EGFdvSBrdvy6+nYtee84zz8955Rks/Oa3uWL2LBZ+89u85lVnlptc5VppO0wD3gb84W6Ox8Z3au34wk2f5K47FnHiS1/MwyuXcflll5aekgr676d/yd333sf5rz7rmXNXzJ7F3fd+n4veNJd7lt3HFbN77lJSD53Mvo5B6bnVLCJuAG7MzLt289qXMvNPx/oCt5ppd9xqpt3ZH1vNZv/mH/WVOTet+upAGu492w6ZOepGwz0JXkkatFqqPbeaSWqKPyMkSQUMcsdCPwxfSU1pZbeDJGkcWPlKaoo9X0kqwJ6vJBVQS8/X8JXUlPG+R/n+4oKbJBVg5SupKS64SVIB9nwlqYBadjvY85WkAqx8JTXFnq8kFVDLVjPDV1JTXHCTpAJccJMkjcrKV1JTXHCTpAJccJOkAmqpfO35SlIBVr6SmlLLbgfDV1JTOvZ8JWnw6ohew1dSY1xwkySNyspXUlNqqXwNX0lN8SILSSqglsrXnq+kpmSf/40lIj4bERsi4kcjzk2JiMUR8WD371FjjWP4StLe+RxwwS7n5gFLMvMEYEn3eU+Gr6SmZGZfxx6MfwewaZfTM4H53cfzgUvGGseer6Sm9NvzjYghYGjEqeHMHB7jY9Myc2338Tpg2ljfY/hKakq/ux26QTtW2Pb6fEbEmJOw7SBJ/VsfEdMBun83jPUBw1dSUzpkX8c+WgTM6T6eAywc6wO2HSQ1ZbxvKRkRNwPnAMdExGrgb4FrgQURMRdYBcwaaxzDV1JTxvuWkpn55lFeOm9vxjF8JTWllpup2/OVpAKsfCU1xV+ykKQCamk7GL6SmmLlK0kF1FL5uuAmSQVY+Upqim0HSSqglraD4SupKZmd0lPYI/Z8JakAK19JTanlBzQNX0lN8afjJakAK19JKqCWytcFN0kqwMpXUlO8yEKSCvAiC0kqoJaer+ErqSm17HZwwU2SCrDyldQU2w6SVIC7HSSpgFoqX3u+klSAla+kptSy28HwldSUWtoOhq+kprjgJkkF1HJ5sQtuklSAla+kpth2kKQCXHCTpAJq6fkavpKaUkvl64KbJBVg5SupKbVUvoavpKbUEb0Qtfwr0YKIGMrM4dLz0IHF/y+em+z5DtZQ6QnogOT/F89Bhq8kFWD4SlIBhu9g2dfT7vj/xXOQC26SVICVryQVYPgOSERcEBH/GREPRcS80vNReRHx2YjYEBE/Kj0XDZ7hOwARMRH4JHAhcBLw5og4qeysdAD4HHBB6UmoDMN3ME4HHsrMlZm5FbgFmFl4TiosM+8ANpWeh8owfAfjWODnI56v7p6T9Bxl+EpSAYbvYKwBXjTi+YzuOUnPUYbvYNwLnBARx0fEJOBSYFHhOUkqyPAdgMzcDlwJ/BuwAliQmQ+UnZVKi4ibgbuBEyNidUTMLT0nDY5XuElSAVa+klSA4StJBRi+klSA4StJBRi+klSA4StJBRi+klSA4StJBfwvsNWiws2dCroAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Y_test_tensor.round(), Y_hat.round(), output_dict=True)\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ND_0GMV7xAGR",
        "outputId": "87fa135a-cd49-47a0-ad0a-4d560b8918e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0.0            0.976744  0.976744  0.976744   43.000000\n",
              "1.0            0.985915  0.985915  0.985915   71.000000\n",
              "accuracy       0.982456  0.982456  0.982456    0.982456\n",
              "macro avg      0.981330  0.981330  0.981330  114.000000\n",
              "weighted avg   0.982456  0.982456  0.982456  114.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ade0eabd-b566-434b-bdd2-fc4087307a89\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>43.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>0.985915</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>71.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.982456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.981330</td>\n",
              "      <td>0.981330</td>\n",
              "      <td>0.981330</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ade0eabd-b566-434b-bdd2-fc4087307a89')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ade0eabd-b566-434b-bdd2-fc4087307a89 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ade0eabd-b566-434b-bdd2-fc4087307a89');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2biWGowxXs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}